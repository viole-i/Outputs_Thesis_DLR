{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright 2016-2018  Flensburg University of Applied Sciences,\n",
    "# Europa-Universität Flensburg,\n",
    "# Centre for Sustainable Energy Systems\n",
    "\n",
    "\n",
    "# This program is free software; you can redistribute it and/or\n",
    "# modify it under the terms of the GNU Affero General Public License as\n",
    "# published by the Free Software Foundation; either version 3 of the\n",
    "# License, or (at your option) any later version.\n",
    "\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "# File description for read-the-docs\n",
    "\"\"\" This module contains functions for calculating representative days/weeks\n",
    "based on a pyPSA network object. It is designed to be used for the `lopf`\n",
    "method. Essentially the tsam package\n",
    "( https://github.com/FZJ-IEK3-VSA/tsam ), which is developed by \n",
    "Leander Kotzur is used.\n",
    "Remaining questions/tasks:\n",
    "- Does it makes sense to cluster normed values?\n",
    "- Include scaling method for yearly sums\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "### we are going to copy and manipulate that here under\n",
    "#import tsam.timeseriesaggregation as tsam\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "__copyright__ = (\"Flensburg University of Applied Sciences, \"\n",
    "                 \"Europa-Universität Flensburg, \"\n",
    "                 \"Centre for Sustainable Energy Systems\")\n",
    "__license__ = \"GNU Affero General Public License Version 3 (AGPL-3.0)\"\n",
    "__author__ = \"Simon Hilpert\"\n",
    "\n",
    "\n",
    "def snapshot_clustering(network, how='daily', clusters=10, clusterMethod = 'hierarchical',normed=False):\n",
    "\n",
    "    network, snapshot_map, day_map = run(network=network.copy(), n_clusters=clusters,\n",
    "                  how=how, normed=normed, clusterMethod = clusterMethod)\n",
    "    return network, snapshot_map, day_map\n",
    "\n",
    "\n",
    "def tsam_cluster(timeseries_df, typical_periods=10, how='daily', clusterMethod = 'hierarchical'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with timeseries to cluster\n",
    "    \n",
    "    clusterMethod : 'hierarchical', 'k_means', 'k_medoids', 'hierarchicalwithpeaks'\n",
    "    Returns\n",
    "    -------\n",
    "    timeseries : pd.DataFrame\n",
    "        Clustered timeseries\n",
    "    \"\"\"\n",
    "    if how == 'hourly':\n",
    "        hours = 1    \n",
    "    if how == 'daily':\n",
    "        hours = 24\n",
    "    if how == 'weekly':\n",
    "        hours = 168\n",
    "    \n",
    "    extremePeriodMethod = 'None'\n",
    "    clusterMethodadd = ''\n",
    "    \n",
    "    ###########################################################    \n",
    "    #URI: This adds peaks\n",
    "    #peakloadcol = '0'\n",
    "    #peakwindcol = '0 wind'\n",
    "    #peaksolarcol = '0 solar'\n",
    "    if clusterMethod == 'hierarchicalwithpeaks':\n",
    "    \n",
    "        #URI: We find the day with more peaks and pick one of the LOADS with such.\n",
    "        #Get indices of the maximums\n",
    "        loading = timeseries_df.filter(regex='0$|1$|2$|3$|4$|5$|6$|7$|8$|9$',axis=1)\n",
    "        loading_max = loading.idxmax()\n",
    "        #Get rid of the hours\n",
    "        #loading_aux = loading.apply(lambda x: str(x.year) + \"-\" + str(x.month) + \"-\" + str(x.day))\n",
    "        loading_aux=loading_max.apply(lambda x: x.strftime(\"%y-%m-%d\"))\n",
    "        #find the day that is peak for most of the loads\n",
    "        #Notice: We just pick the fist date, could be other days with the same maximums\n",
    "        #This comes from: https://stackoverflow.com/questions/6987285/python-find-the-item-with-maximum-occurrences-in-a-list\n",
    "        from collections import defaultdict\n",
    "        from operator import itemgetter\n",
    "        c = defaultdict(int)\n",
    "        for i in loading_aux:\n",
    "            c[i] += 1\n",
    "        peakloadday=max(c.items(), key=itemgetter(1))\n",
    "        for idx, val in enumerate(loading_aux):\n",
    "            if val == peakloadday[0]:\n",
    "                peakloadcol = loading_aux.index.values[idx]\n",
    "                break\n",
    "        \n",
    "        \n",
    "        #URI: We find the day with more peaks and pick one of the WIND generators with such.\n",
    "        #Get indices of the maximums\n",
    "        wind = timeseries_df.filter(regex='wind$', axis=1)\n",
    "        wind_max = wind.idxmax()\n",
    "        #Get rid of the hours\n",
    "        #loading_aux = loading.apply(lambda x: str(x.year) + \"-\" + str(x.month) + \"-\" + str(x.day))\n",
    "        wind_aux=wind_max.apply(lambda x: x.strftime(\"%y-%m-%d\"))\n",
    "        #find the day that is peak for most of the loads\n",
    "        #Notice: We just pick the fist date, could be other days with the same maximums\n",
    "        #This comes from: https://stackoverflow.com/questions/6987285/python-find-the-item-with-maximum-occurrences-in-a-list\n",
    "        from collections import defaultdict\n",
    "        from operator import itemgetter\n",
    "        c = defaultdict(int)\n",
    "        for i in wind_aux:\n",
    "            c[i] += 1\n",
    "        peakwindday=max(c.items(), key=itemgetter(1))\n",
    "        for idx, val in enumerate(wind_aux):\n",
    "            if val == peakwindday[0]:\n",
    "                peakwindcol = wind_aux.index.values[idx]\n",
    "                break\n",
    "        \n",
    "        \n",
    "        \n",
    "        #URI: We find the day with more peaks and pick one of the SOLAR generators with such.\n",
    "        #Get indices of the maximums\n",
    "        solar = timeseries_df.filter(regex='solar$', axis=1)\n",
    "        solar_max = solar.idxmax()\n",
    "        #Get rid of the hours\n",
    "        #loading_aux = loading.apply(lambda x: str(x.year) + \"-\" + str(x.month) + \"-\" + str(x.day))\n",
    "        solar_aux=solar_max.apply(lambda x: x.strftime(\"%y-%m-%d\"))\n",
    "        #find the day that is peak for most of the loads\n",
    "        #Notice: We just pick the fist date, could be other days with the same maximums\n",
    "        #This comes from: https://stackoverflow.com/questions/6987285/python-find-the-item-with-maximum-occurrences-in-a-list\n",
    "        from collections import defaultdict\n",
    "        from operator import itemgetter\n",
    "        c = defaultdict(int)\n",
    "        for i in solar_aux:\n",
    "            c[i] += 1\n",
    "        peaksolarday=max(c.items(), key=itemgetter(1))\n",
    "        for idx, val in enumerate(solar_aux):\n",
    "            if val == peaksolarday[0]:\n",
    "                #This is not the column with the overall maximum, just the first column with the maximum at the place\n",
    "                #in time with more maximums\n",
    "                peaksolarcol = solar_aux.index.values[idx]\n",
    "                break\n",
    "                         \n",
    "        clusterMethod = 'hierarchical'\n",
    "        clusterMethodadd = 'withpeaks'\n",
    "        extremePeriodMethod = 'new_cluster_center'\n",
    "        addPeakMax = [peakloadcol,peakwindcol,peaksolarcol]\n",
    "    else:\n",
    "        addPeakMax = None\n",
    "     \n",
    "    ##################################################################\n",
    "    #URI: This is the original method=hierarchical\n",
    "    if clusterMethod == 'hierarchical':\n",
    "        aggregation = TimeSeriesAggregation(\n",
    "            timeseries_df,\n",
    "            noTypicalPeriods=typical_periods,\n",
    "            rescaleClusterPeriods=False,\n",
    "            hoursPerPeriod=hours,\n",
    "            clusterMethod= clusterMethod, #averaging, k_means, k_medoids, hierarchical\n",
    "            extremePeriodMethod = extremePeriodMethod , #'None', 'append', 'new_cluster_center', 'replace_cluster_center'\n",
    "            addPeakMax = addPeakMax)\n",
    "        \n",
    "        timeseries = aggregation.createTypicalPeriods()\n",
    "        #URI: Better take the whole thing\n",
    "        timeseries_new =aggregation.predictOriginalData()\n",
    "        cluster_weights = aggregation.clusterPeriodNoOccur\n",
    "    \n",
    "        \n",
    "        # get the medoids/ the clusterCenterIndices\n",
    "        clusterCenterIndices = aggregation.clusterCenterIndices\n",
    "        \n",
    "        #URI: and add the peak periods\n",
    "        if not extremePeriodMethod is 'None':\n",
    "            clusterOrder = aggregation.clusterOrder\n",
    "            for i in range(len(clusterCenterIndices),len(cluster_weights.keys())):\n",
    "                clusterCenterIndices.append(np.where(clusterOrder == i)[0][0]) \n",
    "        \n",
    "        # get all index for every hour of that day of the clusterCenterIndices\n",
    "        start = []\n",
    "        # get the first hour of the clusterCenterIndices (days start with 0)\n",
    "        for i in clusterCenterIndices:\n",
    "            start.append(i * hours)\n",
    "    \n",
    "        # get a list with all hours belonging to the clusterCenterIndices\n",
    "        nrhours = []\n",
    "        for j in start:\n",
    "            nrhours.append(j)\n",
    "            x = 1\n",
    "            while x < hours:\n",
    "                j = j + 1\n",
    "                nrhours.append(j)\n",
    "                x = x + 1\n",
    "    \n",
    "        # get the origial Datetimeindex\n",
    "        dates = timeseries_df.iloc[nrhours].index\n",
    "    \n",
    "    \n",
    "    ######################################################\n",
    "    #URI: This is them method=k_means or k_medoids\n",
    "    elif clusterMethod == 'k_means' or clusterMethod == 'k_medoids':\n",
    "        aggregation = TimeSeriesAggregation(\n",
    "            timeseries_df,\n",
    "            noTypicalPeriods=typical_periods,\n",
    "            rescaleClusterPeriods=False,\n",
    "            hoursPerPeriod=hours,\n",
    "            clusterMethod=clusterMethod)#averaging, k_means, k_medoids, hierarchical\n",
    "            #solver='gurobi' #This was made available in version 1.0.0\n",
    "    \n",
    "        timeseries = aggregation.createTypicalPeriods()\n",
    "        #URI: Better take the whole thing\n",
    "        timeseries_new =aggregation.predictOriginalData()\n",
    "        cluster_weights = aggregation.clusterPeriodNoOccur\n",
    "    \n",
    "        \n",
    "        # get a representative asclusterCenterIndices\n",
    "        #clusterCenterIndices = aggregation.clusterCenterIndices\n",
    "        clusterCenterIndices = []\n",
    "        for i in cluster_weights.keys():\n",
    "            clusterCenterIndices.append(np.argmax(aggregation.clusterOrder == i))\n",
    "        \n",
    "        #URI: and add the peak periods\n",
    "        #clusterOrder = aggregation.clusterOrder\n",
    "        #for i in range(len(clusterCenterIndices),len(cluster_weights.keys())):\n",
    "        #    clusterCenterIndices.append(np.where(clusterOrder == i)[0][0]) \n",
    "        \n",
    "        # get all index for every hour of that day of the clusterCenterIndices\n",
    "        start = []\n",
    "        # get the first hour of the clusterCenterIndices (days start with 0)\n",
    "        for i in clusterCenterIndices:\n",
    "            start.append(i * hours)\n",
    "    \n",
    "        # get a list with all hours belonging to the clusterCenterIndices\n",
    "        nrhours = []\n",
    "        for j in start:\n",
    "            nrhours.append(j)\n",
    "            x = 1\n",
    "            while x < hours:\n",
    "                j = j + 1\n",
    "                nrhours.append(j)\n",
    "                x = x + 1\n",
    "    \n",
    "        # get the origial Datetimeindex\n",
    "        dates = timeseries_df.iloc[nrhours].index\n",
    "        \n",
    "    ##################################################################\n",
    "    #URI: This is the original method=hierarchical\n",
    "    elif clusterMethod == 'chronological':\n",
    "        aggregation = TimeSeriesAggregation(\n",
    "            timeseries_df,\n",
    "            noTypicalPeriods=typical_periods,\n",
    "            rescaleClusterPeriods=False,\n",
    "            hoursPerPeriod=hours,\n",
    "            clusterMethod='hierarchical', #really chronological, but we will manipulate what tsam.TimeSeriesAgregation(...hierarchical...) does\n",
    "            extremePeriodMethod = extremePeriodMethod , #'None', 'append', 'new_cluster_center', 'replace_cluster_center'\n",
    "            addPeakMax = addPeakMax)\n",
    "        \n",
    "        timeseries = aggregation.createTypicalPeriods()\n",
    "        #URI: Better take the whole thing\n",
    "        timeseries_new =aggregation.predictOriginalData()\n",
    "        cluster_weights = aggregation.clusterPeriodNoOccur\n",
    "    \n",
    "        \n",
    "        # get the medoids/ the clusterCenterIndices\n",
    "        clusterCenterIndices = aggregation.clusterCenterIndices\n",
    "        \n",
    "        #URI: and add the peak periods\n",
    "        if not extremePeriodMethod is 'None':\n",
    "            clusterOrder = aggregation.clusterOrder\n",
    "            for i in range(len(clusterCenterIndices),len(cluster_weights.keys())):\n",
    "                clusterCenterIndices.append(np.where(clusterOrder == i)[0][0]) \n",
    "        \n",
    "        # get all index for every hour of that day of the clusterCenterIndices\n",
    "        start = []\n",
    "        # get the first hour of the clusterCenterIndices (days start with 0)\n",
    "        for i in clusterCenterIndices:\n",
    "            start.append(i * hours)\n",
    "    \n",
    "        # get a list with all hours belonging to the clusterCenterIndices\n",
    "        nrhours = []\n",
    "        for j in start:\n",
    "            nrhours.append(j)\n",
    "            x = 1\n",
    "            while x < hours:\n",
    "                j = j + 1\n",
    "                nrhours.append(j)\n",
    "                x = x + 1\n",
    "    \n",
    "        # get the origial Datetimeindex\n",
    "        dates = timeseries_df.iloc[nrhours].index\n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################\n",
    "    #URI: this is to get the data as csv\n",
    "    directory = '/home/raventos/tsamdata' + clusterMethod + clusterMethodadd + '/'\n",
    "    timeseries_df.to_csv(directory + 'rawPeriods' + str(typical_periods) + '.csv')\n",
    "    timeseries.to_csv(directory + 'typPeriods_unscaled' + str(typical_periods) + '.csv')\n",
    "    timeseries_new.to_csv(directory + 'predictedPeriods_unscaled' + str(typical_periods) + '.csv')\n",
    "    indexMatching=aggregation.indexMatching()\n",
    "    indexMatching.to_csv(directory + 'indexMatching' + str(typical_periods) + '.csv')\n",
    "    clusterOrder = aggregation.clusterOrder.astype(int)\n",
    "    np.savetxt(directory + 'clusterOrder' + str(typical_periods) + '.csv', clusterOrder, fmt='%i', delimiter=\",\")\n",
    "    cCI=pd.DataFrame(clusterCenterIndices,columns=['index'])\n",
    "    cCI.to_csv(directory + 'clusterCenterIndices' + str(typical_periods) + '.csv')\n",
    "    noOccur=pd.DataFrame(cluster_weights, index=['noOccur'])\n",
    "    noOccur.to_csv(directory + 'noOccurrances' + str(typical_periods) + '.csv')\n",
    "    aggregation.accuracyIndicators().to_csv(directory + 'indicators_unscaled' + str(typical_periods) + '.csv')\n",
    "    np.savetxt(directory + 'dates' + str(typical_periods) + '.csv',dates.strftime(\"%Y-%m-%d %X\"), fmt=\"%s\", delimiter=\",\")\n",
    "\n",
    "    #URI: This is for Bruno's coupling\n",
    "    #dates = pd.read_csv(main_folder + method + str(count) + '/tsamdata/dates' + str(count) +'.csv', index_col = 0)\n",
    "    dates2 =[]\n",
    "    for row in indexMatching.index:\n",
    "        dates2.append(dates[indexMatching.loc[row,'PeriodNum']*hours + indexMatching.loc[row,'TimeStep']])\n",
    "    snapshot_map= pd.Series(dates2, index=indexMatching.index)\n",
    "    \n",
    "    #URI: Just checking\n",
    "    snapshot_map.to_csv(directory + 'snapshot_map' + str(typical_periods) + '.csv')\n",
    "    #print([peakloadcol,peakwindcol,peaksolarcol])\n",
    "    #print(clusterCenterIndices)\n",
    "    #print([peakloadday,peakwindday,peaksolarday])\n",
    "    #print(dates)\n",
    "    #print(cluster_weights)\n",
    "    #print(aggregation.indexMatching())\n",
    "    return timeseries_new, cluster_weights, dates, hours, snapshot_map, clusterOrder\n",
    "\n",
    "\n",
    "def run(network, n_clusters=None, how='daily',\n",
    "        normed=False, clusterMethod = 'hierarchical'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # reduce storage costs due to clusters\n",
    "    #URI: This made nothing\n",
    "    #network.cluster = True\n",
    "\n",
    "    # calculate clusters\n",
    "    tsam_pre, divisor= prepare_pypsa_timeseries(network, normed)\n",
    "    tsam_ts, cluster_weights, dates, hours, snapshot_map, day_map = tsam_cluster(\n",
    "            tsam_pre, typical_periods=n_clusters,\n",
    "            how=how, clusterMethod = clusterMethod)\n",
    "    #URI: It used to be (wrong): how = 'daily'\n",
    "    \n",
    "    #URI: Here we do the scaling and get the scaled data\n",
    "    \n",
    "    tsam_ts = rescaleData(tsam_pre,tsam_ts,divisor)\n",
    "    directory = '/home/raventos/tsamdata' + clusterMethod +'/'\n",
    "    tsam_ts.to_csv(directory + 'predictedPeriods' + str(n_clusters) + '.csv')\n",
    "    typPeriods = tsam_ts.loc[dates]\n",
    "    typPeriods.to_csv(directory + 'typPeriods' + str(n_clusters) + '.csv')\n",
    "    accIndicators = accuracyIndicators(tsam_ts,tsam_pre)\n",
    "    accIndicators.to_csv(directory + 'indicators' + str(n_clusters) + '.csv')\n",
    "    \n",
    "\n",
    "    update_data_frames(network, tsam_ts, divisor, cluster_weights, dates, hours, normed)\n",
    "\n",
    "    return network, snapshot_map, day_map\n",
    "\n",
    "\n",
    "def prepare_pypsa_timeseries(network, normed=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if normed:\n",
    "        normed_loads = network.loads_t.p_set / network.loads_t.p_set.max()\n",
    "        normed_renewables = network.generators_t.p_max_pu\n",
    "        #URI: This divisor will simplify the update\n",
    "        divisor= len(normed_renewables.columns)\n",
    "        df = pd.concat([normed_renewables, normed_loads], axis=1)\n",
    "    else:\n",
    "        loads = network.loads_t.p_set\n",
    "        renewables = network.generators_t.p_max_pu #URI:previously .p_set\n",
    "        #URI: This divisor will simplify the update\n",
    "        divisor= len(renewables.columns)\n",
    "        df = pd.concat([renewables, loads], axis=1)\n",
    "\n",
    "    return df, divisor\n",
    "\n",
    "\n",
    "def update_data_frames(network, tsam_ts, divisor, cluster_weights, dates, hours, normed=False):\n",
    "    \"\"\" Updates the snapshots, snapshots weights and the dataframes based on\n",
    "    the original data in the network and the medoids created by clustering\n",
    "    these original data.\n",
    "    Parameters\n",
    "    -----------\n",
    "    network : pyPSA network object\n",
    "    cluster_weights: dictionary\n",
    "    dates: Datetimeindex\n",
    "    Returns\n",
    "    -------\n",
    "    network\n",
    "    \"\"\"\n",
    "    network.snapshot_weightings = network.snapshot_weightings.loc[dates]\n",
    "    network.snapshots = network.snapshot_weightings.index\n",
    "\n",
    "    # set new snapshot weights from cluster_weights\n",
    "    snapshot_weightings = []\n",
    "    for i in cluster_weights.values():\n",
    "        x = 0\n",
    "        while x < hours:\n",
    "            snapshot_weightings.append(i)\n",
    "            x += 1\n",
    "    for i in range(len(network.snapshot_weightings)):\n",
    "        network.snapshot_weightings[i] = snapshot_weightings[i]\n",
    "\n",
    "    # put the snapshot in the right order\n",
    "    #network.snapshots.sort_values() \n",
    "    #network.snapshot_weightings.sort_index() \n",
    "    #URI: If we want it to really work:\n",
    "    network.snapshots=network.snapshots.sort_values()\n",
    "    network.snapshot_weightings=network.snapshot_weightings.sort_index()  \n",
    "    \n",
    "    #URI: Need to separate generators from load\n",
    "    #URI: and add the normed case\n",
    "    if normed:\n",
    "            network.generators_t.p_max_pu = tsam_ts.iloc[:,:divisor] #URI:previously .p_set\n",
    "            network.loads_t.p_set = tsam_ts.iloc[:,divisor:]*network.loads_t.p_set.max()\n",
    "    else:\n",
    "        network.generators_t.p_max_pu = tsam_ts.iloc[:,:divisor] #URI:previously .p_set\n",
    "        network.loads_t.p_set = tsam_ts.iloc[:,divisor:] \n",
    "    \n",
    "    #URI: We don't want to keep numbers that are too small    \n",
    "    network.generators_t.p_max_pu.where(lambda df: df>0.000001, other=0., inplace=True)\n",
    "    network.loads_t.p_set.where(lambda df: df>0.000001, other=0., inplace=True)\n",
    "    \n",
    "    return network\n",
    "\n",
    "\n",
    "   \n",
    "def rescaleData(tsam_pre,tsam_ts,divisor):\n",
    "    \n",
    "    #first for the renewables\n",
    "    for i in range(divisor):\n",
    "        diff = 1.\n",
    "        a=0\n",
    "        while diff > 0.000001 and a < 20:\n",
    "            scal = tsam_pre.iloc[:,i].sum()/tsam_ts.iloc[:,i].sum()\n",
    "            k=(tsam_ts.iloc[:,i]>1.).sum()\n",
    "            if scal > 1.0 and k > 0:\n",
    "                #print(scal, k)\n",
    "                scal = scal + k*(scal-1)\n",
    "            tsam_ts.iloc[:,i]= tsam_ts.iloc[:,i]*scal\n",
    "            diff = tsam_ts.iloc[:,i].max() - 1.\n",
    "            for ii in range(len(tsam_ts.index)):\n",
    "                if(tsam_ts.iloc[ii,i] > 1.): \n",
    "                    tsam_ts.iloc[ii,i]=1.\n",
    "            a = a+1\n",
    "        if a== 20:\n",
    "            print ('Column ' + str(tsam_ts.columns[i]) + ' could not be scaled in 20 itereations')\n",
    "            #print(diff)\n",
    "            #print(tsam_ts.iloc[:,i].idxmax())\n",
    "    \n",
    "    #then for the load\n",
    "    for i in range(divisor,len(tsam_pre.columns)):\n",
    "        scal = tsam_pre.iloc[:,i].sum()/tsam_ts.iloc[:,i].sum()\n",
    "        tsam_ts.iloc[:,i]=tsam_ts.iloc[:,i]*scal\n",
    "    \n",
    "    return tsam_ts\n",
    "\n",
    "#This is taken from the tsam package by Leander Kotzur, but we want to use it here after the scaling\n",
    "def accuracyIndicators(tsam_ts,tsam_pre):\n",
    "    indicatorRaw = {\n",
    "        'RMSE': {},\n",
    "        'RMSE_duration': {},\n",
    "        'MAE': {}}\n",
    "\n",
    "    for column in tsam_pre.columns:\n",
    "        origTS = tsam_pre[column]\n",
    "        predTS = tsam_ts[column]\n",
    "        indicatorRaw['RMSE'][column] = np.sqrt(\n",
    "            mean_squared_error(origTS, predTS))\n",
    "        indicatorRaw['RMSE_duration'][column] = np.sqrt(mean_squared_error(\n",
    "            origTS.sort_values(ascending=False).reset_index(drop=True),\n",
    "            predTS.sort_values(ascending=False).reset_index(drop=True)))\n",
    "        indicatorRaw['MAE'][column] = mean_absolute_error(origTS, predTS)\n",
    "\n",
    "    return pd.DataFrame(indicatorRaw)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#Here we copy and manipulate tsam.TimeSeriesAggregation\n",
    "#We mark the manipulations by #URI:\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov 22 23:25:37 2016\n",
    "@author: Kotzur\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# max iterator while resacling cluster profiles\n",
    "MAX_ITERATOR = 20\n",
    "\n",
    "# tolerance while rescaling cluster periods to meet the annual sum of the original profile\n",
    "TOLERANCE = 1e-6\n",
    "\n",
    "def unstackToPeriods(timeSeries, timeStepsPerPeriod):\n",
    "    \"\"\"\n",
    "    Extend the timeseries to an integer multiple of the period length and\n",
    "    groups the time series to the periods.\n",
    "    Parameters\n",
    "    -----------\n",
    "    timeSeries\n",
    "        pandas.DataFrame()\n",
    "    timeStepsPerPeriod: integer, required\n",
    "        The number of discrete timesteps which describe one period.\n",
    "    Returns\n",
    "    -------\n",
    "    unstackedTimeSeries\n",
    "        pandas.DataFrame() which is stacked such that each row represents a\n",
    "        candidate period\n",
    "    timeIndex\n",
    "        pandas.Series.index which is the modification of the original\n",
    "        timeseriesindex in case an integer multiple was created\n",
    "    \"\"\"\n",
    "    # init new grouped timeindex\n",
    "    unstackedTimeSeries = timeSeries.copy()\n",
    "\n",
    "    # initalize new indices\n",
    "    periodIndex = []\n",
    "    stepIndex = []\n",
    "\n",
    "    # extend to inger multiple of period length\n",
    "    if len(timeSeries) % timeStepsPerPeriod == 0:\n",
    "        attached_timesteps = 0\n",
    "    else:\n",
    "        # calculate number of timesteps which get attached\n",
    "        attached_timesteps = timeStepsPerPeriod - \\\n",
    "                             len(timeSeries) % timeStepsPerPeriod\n",
    "\n",
    "        # take these from the head of the original time series\n",
    "        rep_data = unstackedTimeSeries.head(attached_timesteps)\n",
    "\n",
    "        # append them at the end of the time series\n",
    "        unstackedTimeSeries = unstackedTimeSeries.append(rep_data,\n",
    "                                                         ignore_index=False)\n",
    "\n",
    "    # create period and step index\n",
    "    for ii in range(0, len(unstackedTimeSeries)):\n",
    "        periodIndex.append(int(ii / timeStepsPerPeriod))\n",
    "        stepIndex.append(\n",
    "            ii - int(ii / timeStepsPerPeriod) * timeStepsPerPeriod)\n",
    "\n",
    "    # save old index\n",
    "    timeIndex = copy.deepcopy(unstackedTimeSeries.index)\n",
    "\n",
    "    # create new double index and unstack the time series\n",
    "    unstackedTimeSeries.index = pd.MultiIndex.from_arrays([stepIndex,\n",
    "                                                           periodIndex],\n",
    "                                                          names=['TimeStep',\n",
    "                                                                 'PeriodNum'])\n",
    "    unstackedTimeSeries = unstackedTimeSeries.unstack(level='TimeStep')\n",
    "\n",
    "    return unstackedTimeSeries, timeIndex\n",
    "\n",
    "\n",
    "def aggregatePeriods(candidates, n_clusters=8,\n",
    "                     n_iter=100, clusterMethod='k_means', solver='glpk'):\n",
    "    '''\n",
    "    Clusters the data based on one of the cluster methods:\n",
    "        'averaging','k_means','exact k_medoid' or 'hierarchical'\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidates: np.ndarray, required\n",
    "        Dissimilarity matrix where each row represents a candidate\n",
    "    n_clusters: int, optional (default: 8)\n",
    "        Number of aggregated cluster.\n",
    "    n_iter: int, optional (default: 10)\n",
    "        Only required for the number of starts of the k-mean algorithm.\n",
    "    clusterMethod: str, optional (default: 'k_means')\n",
    "        Chosen clustering algorithm. Possible values are\n",
    "        'averaging','k_means','exact k_medoid' or 'hierarchical'\n",
    "    '''\n",
    "\n",
    "    if clusterMethod == 'hierarchical':\n",
    "        clusterCenterIndices = []\n",
    "    else:\n",
    "        clusterCenterIndices = None\n",
    "\n",
    "    # cluster the data\n",
    "    if clusterMethod == 'averaging':\n",
    "        n_sets = len(candidates)\n",
    "        if n_sets % n_clusters == 0:\n",
    "            cluster_size = int(n_sets / n_clusters)\n",
    "            clusterOrder = [\n",
    "                [n_cluster] *\n",
    "                cluster_size for n_cluster in range(n_clusters)]\n",
    "        else:\n",
    "            cluster_size = int(n_sets / n_clusters)\n",
    "            clusterOrder = [\n",
    "                [n_cluster] *\n",
    "                cluster_size for n_cluster in range(n_clusters)]\n",
    "            clusterOrder.append([n_clusters - 1] *\n",
    "                                int(n_sets - cluster_size * n_clusters))\n",
    "        clusterOrder = np.hstack(np.array(clusterOrder))\n",
    "        clusterCenters = []\n",
    "        for clusterNum in np.unique(clusterOrder):\n",
    "            indice = np.where(clusterOrder == clusterNum)\n",
    "            currentMean = candidates[indice].mean(axis=0)\n",
    "            clusterCenters.append(currentMean)\n",
    "\n",
    "    if clusterMethod == 'k_means':\n",
    "        from sklearn.cluster import KMeans\n",
    "        k_means = KMeans(\n",
    "            n_clusters=n_clusters,\n",
    "            max_iter=1000,\n",
    "            n_init=n_iter,\n",
    "            tol=1e-4)\n",
    "\n",
    "        clusterOrder = k_means.fit_predict(candidates)\n",
    "        clusterCenters = k_means.cluster_centers_\n",
    "\n",
    "    elif clusterMethod == 'k_medoids':\n",
    "        from tsam.utils.k_medoids_exact import KMedoids\n",
    "        k_medoid = KMedoids(n_clusters=n_clusters, solver=solver)\n",
    "\n",
    "        clusterOrder = k_medoid.fit_predict(candidates)\n",
    "        clusterCenters = k_medoid.cluster_centers_\n",
    "    #\n",
    "\n",
    "    elif clusterMethod == 'hierarchical':\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        #URI: In fact chronological, i.e. we will cluster adjacent hours\n",
    "        T=len(candidates)\n",
    "        connectivity = []\n",
    "        for i in range(T):\n",
    "            vector=[]\n",
    "            for j in range(T):\n",
    "                if j==(i+1)%T or j==(i-1)%T:\n",
    "                    vector.append(1)\n",
    "                else:\n",
    "                    vector.append(0)\n",
    "            connectivity.append(vector)\n",
    "        \n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters, linkage='ward',connectivity=connectivity)\n",
    "\n",
    "        clusterOrder = clustering.fit_predict(candidates)\n",
    "\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        # set cluster center as medoid\n",
    "        clusterCenters = []\n",
    "        for clusterNum in np.unique(clusterOrder):\n",
    "            indice = np.where(clusterOrder == clusterNum)\n",
    "            innerDistMatrix = euclidean_distances(candidates[indice])\n",
    "            mindistIdx = np.argmin(innerDistMatrix.sum(axis=0))\n",
    "            clusterCenters.append(candidates[indice][mindistIdx])\n",
    "            clusterCenterIndices.append(indice[0][mindistIdx])\n",
    "\n",
    "    return clusterCenters, clusterCenterIndices, clusterOrder\n",
    "\n",
    "\n",
    "class TimeSeriesAggregation(object):\n",
    "    '''\n",
    "    Clusters time series data to typical periods.\n",
    "    '''\n",
    "    CLUSTER_METHODS = ['averaging', 'k_medoids', 'k_means', 'hierarchical']\n",
    "\n",
    "    EXTREME_PERIOD_METHODS = [\n",
    "        'None',\n",
    "        'append',\n",
    "        'new_cluster_center',\n",
    "        'replace_cluster_center']\n",
    "\n",
    "    def __init__(self, timeSeries, resolution=None, noTypicalPeriods=10,\n",
    "                 hoursPerPeriod=24, clusterMethod='hierarchical',\n",
    "                 evalSumPeriods=False, sortValues=False, sameMean=False,\n",
    "                 rescaleClusterPeriods=True, weightDict=None,\n",
    "                 extremePeriodMethod='None', solver='glpk',\n",
    "                 addPeakMin=None,\n",
    "                 addPeakMax=None,\n",
    "                 addMeanMin=None,\n",
    "                 addMeanMax=None):\n",
    "        '''\n",
    "        Initialize the periodly clusters.\n",
    "        Parameters\n",
    "        -----------\n",
    "        timeSeries: pandas.DataFrame() or dict, required\n",
    "            DataFrame with the datetime as index and the relevant\n",
    "            time series parameters as columns.\n",
    "        resolution: float, optional, default: delta_T in timeSeries\n",
    "            Resolution of the time series in hours [h]. If timeSeries is a\n",
    "            pandas.DataFrame() the resolution is derived from the datetime\n",
    "            index.\n",
    "        hoursPerPeriod: int, optional, default: 24\n",
    "            Value which defines the length of a cluster period.\n",
    "        noTypicalPeriods: int, optional, default: 10\n",
    "            Number of typical Periods - equivalent to the number of clusters.\n",
    "        clusterMethod: {'averaging','k_means','k_medoids','hierarchical'},\n",
    "                        optional, default: 'hierarchical'\n",
    "            Chosen clustering method.\n",
    "        evalSumPeriods: boolean, optional, default: False\n",
    "            Boolean if in the clustering process also the averaged periodly values\n",
    "            shall be integrated additional to the periodly profiles as parameters.\n",
    "        sameMean: boolean, optional, default: False\n",
    "            Boolean which is used in the normalization procedure. If true,\n",
    "            all time series get normalized such that they have the same mean value.\n",
    "        sortValues: boolean, optional (default: False)\n",
    "            Boolean if the clustering should be done by the periodly duration\n",
    "            curves (true) or the original shape of the data.\n",
    "        rescaleClusterPeriods: boolean, optional (default: True)\n",
    "            Decides if the cluster Periods shall get rescaled such that their\n",
    "            weighted mean value fits the mean value of the original time\n",
    "            series.\n",
    "        weightDict: dict, optional (default: None )\n",
    "            Dictionary which weights the profiles. It is done by scaling\n",
    "            the time series while the normalization process. Normally all time\n",
    "            series have a scale from 0 to 1. By scaling them, the values get\n",
    "            different distances to each other and with this, they are\n",
    "            differently evaluated while the clustering process.\n",
    "        extremePeriodMethod: {'None','append','new_cluster_center',\n",
    "                           'replace_cluster_center'}, optional, default: 'None'\n",
    "            Method how to integrate extreme Periods (peak demand,\n",
    "                                                  lowest temperature etc.)\n",
    "            into to the typical period profiles.\n",
    "                None: No integration at all.\n",
    "                'append': append typical Periods to cluster centers\n",
    "                'new_cluster_center': add the extreme period as additional cluster\n",
    "                    center. It is checked then for all Periods if they fit better\n",
    "                    to the this new center or their original cluster center.\n",
    "                'replace_cluster_center': replaces the cluster center of the\n",
    "                    cluster where the extreme period belongs to with the periodly\n",
    "                    profile of the extreme period. (Worst case system design)\n",
    "        solver: string, optional (default: 'glpk' )\n",
    "            Solver that is used for k_medoids clustering.\n",
    "        addPeakMin: list, optional, default: []\n",
    "            List of column names which's minimal value shall be added to the\n",
    "            typical periods. E.g.: ['Temperature']\n",
    "        addPeakMax: list, optional, default: []\n",
    "            List of column names which's maximal value shall be added to the\n",
    "            typical periods. E.g. ['EDemand', 'HDemand']\n",
    "        addMeanMin: list, optional, default: []\n",
    "            List of column names where the period with the cumulative minimal value\n",
    "            shall be added to the typical periods. E.g. ['Photovoltaic']\n",
    "        addMeanMax: list, optional, default: []\n",
    "            List of column names where the period with the cumulative maximal value\n",
    "            shall be added to the typical periods.\n",
    "        '''\n",
    "        if addMeanMin is None:\n",
    "            addMeanMin = []\n",
    "        if addMeanMax is None:\n",
    "            addMeanMax = []\n",
    "        if addPeakMax is None:\n",
    "            addPeakMax = []\n",
    "        if addPeakMin is None:\n",
    "            addPeakMin = []\n",
    "        if weightDict is None:\n",
    "            weightDict = {}\n",
    "        self.timeSeries = timeSeries\n",
    "\n",
    "        self.resolution = resolution\n",
    "\n",
    "        self.hoursPerPeriod = hoursPerPeriod\n",
    "\n",
    "        self.noTypicalPeriods = noTypicalPeriods\n",
    "\n",
    "        self.clusterMethod = clusterMethod\n",
    "\n",
    "        self.extremePeriodMethod = extremePeriodMethod\n",
    "\n",
    "        self.evalSumPeriods = evalSumPeriods\n",
    "\n",
    "        self.sortValues = sortValues\n",
    "\n",
    "        self.sameMean = sameMean\n",
    "\n",
    "        self.rescaleClusterPeriods = rescaleClusterPeriods\n",
    "\n",
    "        self.weightDict = weightDict\n",
    "\n",
    "        self.solver = solver\n",
    "\n",
    "        self.addPeakMin = addPeakMin\n",
    "\n",
    "        self.addPeakMax = addPeakMax\n",
    "\n",
    "        self.addMeanMin = addMeanMin\n",
    "\n",
    "        self.addMeanMax = addMeanMax\n",
    "\n",
    "        self._check_init_args()\n",
    "\n",
    "        return\n",
    "\n",
    "    def _check_init_args(self):\n",
    "\n",
    "        # check timeSeries and set it as pandas DataFrame\n",
    "        if not isinstance(self.timeSeries, pd.DataFrame):\n",
    "            if isinstance(self.timeSeries, dict):\n",
    "                self.timeSeries = pd.DataFrame(self.timeSeries)\n",
    "            elif isinstance(self.timeSeries, np.ndarray):\n",
    "                self.timeSeries = pd.DataFrame(self.timeSeries)\n",
    "            else:\n",
    "                raise ValueError('timeSeries has to be of type pandas.DataFrame() ' +\n",
    "                                 'or of type np.array() '\n",
    "                                 'in initialization of object of class ' +\n",
    "                                 type(self).__name__)\n",
    "\n",
    "        # check if extreme periods exist in the dataframe\n",
    "        for peak in self.addPeakMin:\n",
    "            if peak not in self.timeSeries.columns:\n",
    "                raise ValueError(peak + ' listed in \"addPeakMin\"' +\n",
    "                                 ' does not occure as timeSeries column')\n",
    "        for peak in self.addPeakMax:\n",
    "            if peak not in self.timeSeries.columns:\n",
    "                raise ValueError(peak + ' listed in \"addPeakMax\"' +\n",
    "                                 ' does not occure as timeSeries column')\n",
    "        for peak in self.addMeanMin:\n",
    "            if peak not in self.timeSeries.columns:\n",
    "                raise ValueError(peak + ' listed in \"addMeanMin\"' +\n",
    "                                 ' does not occure as timeSeries column')\n",
    "        for peak in self.addMeanMax:\n",
    "            if peak not in self.timeSeries.columns:\n",
    "                raise ValueError(peak + ' listed in \"addMeanMax\"' +\n",
    "                                 ' does not occure as timeSeries column')\n",
    "\n",
    "        # derive resolution from date time index if not provided\n",
    "        if self.resolution is None:\n",
    "            try:\n",
    "                timedelta = self.timeSeries.index[1] - self.timeSeries.index[0]\n",
    "                self.resolution = float(timedelta.total_seconds()) / 3600\n",
    "            except TypeError:\n",
    "                try:\n",
    "                    self.timeSeries.index = pd.to_datetime(self.timeSeries.index)\n",
    "                    timedelta = self.timeSeries.index[1] - self.timeSeries.index[0]\n",
    "                    self.resolution = float(timedelta.total_seconds()) / 3600\n",
    "                except:\n",
    "                    ValueError(\"'resolution' argument has to be nonnegative float or int\" +\n",
    "                               \" or the given timeseries needs a datetime index\")\n",
    "\n",
    "        if not (isinstance(self.resolution, int) or isinstance(self.resolution, float)):\n",
    "            raise ValueError(\"resolution has to be nonnegative float or int\")\n",
    "\n",
    "        # check hoursPerPeriod\n",
    "        if self.hoursPerPeriod is None or self.hoursPerPeriod <= 0 or \\\n",
    "                not isinstance(self.hoursPerPeriod, int):\n",
    "            raise ValueError(\"hoursPerPeriod has to be nonnegative integer\")\n",
    "\n",
    "        # check typical Periods\n",
    "        if self.noTypicalPeriods is None or self.noTypicalPeriods <= 0 or \\\n",
    "                not isinstance(self.noTypicalPeriods, int):\n",
    "            raise ValueError(\"noTypicalPeriods has to be nonnegative integer\")\n",
    "        self.timeStepsPerPeriod = int(self.hoursPerPeriod / self.resolution)\n",
    "        if not self.timeStepsPerPeriod == self.hoursPerPeriod / self.resolution:\n",
    "            raise ValueError('The combination of hoursPerPeriod and the '\n",
    "                             + 'resulution does not result in an integer '\n",
    "                             + 'number of time steps per period')\n",
    "\n",
    "        # check clusterMethod\n",
    "        if self.clusterMethod not in self.CLUSTER_METHODS:\n",
    "            raise ValueError(\"clusterMethod needs to be one of \" +\n",
    "                             \"the following: \" +\n",
    "                             \"{}\".format(self.CLUSTER_METHODS))\n",
    "\n",
    "        # check extremePeriods\n",
    "        if self.extremePeriodMethod not in self.EXTREME_PERIOD_METHODS:\n",
    "            raise ValueError(\"extremePeriodMethod needs to be one of \" +\n",
    "                             \"the following: \" +\n",
    "                             \"{}\".format(self.EXTREME_PERIOD_METHODS))\n",
    "\n",
    "        # check evalSumPeriods\n",
    "        if not isinstance(self.evalSumPeriods, bool):\n",
    "            raise ValueError(\"evalSumPeriods has to be boolean\")\n",
    "        # check sortValues\n",
    "        if not isinstance(self.sortValues, bool):\n",
    "            raise ValueError(\"sortValues has to be boolean\")\n",
    "        # check sameMean\n",
    "        if not isinstance(self.sameMean, bool):\n",
    "            raise ValueError(\"sameMean has to be boolean\")\n",
    "        # check rescaleClusterPeriods\n",
    "        if not isinstance(self.rescaleClusterPeriods, bool):\n",
    "            raise ValueError(\"rescaleClusterPeriods has to be boolean\")\n",
    "\n",
    "    def _normalizeTimeSeries(self, sameMean=False):\n",
    "        '''\n",
    "        Normalizes each time series independently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sameMean: boolean, optional (default: False)\n",
    "            Decides if the time series should have all the same mean value.\n",
    "            Relevant for weighting time series.\n",
    "        Returns\n",
    "        ---------\n",
    "        normalized time series\n",
    "        '''\n",
    "        normalizedTimeSeries = pd.DataFrame()\n",
    "        for column in self.timeSeries:\n",
    "            if not self.timeSeries[column].max() == self.timeSeries[column].min():  # ignore constant timeseries\n",
    "                normalizedTimeSeries[column] = (\n",
    "                                                   self.timeSeries[column] -\n",
    "                                                   self.timeSeries[column].min()) / \\\n",
    "                                               (self.timeSeries[column].max() -\n",
    "                                                self.timeSeries[column].min())\n",
    "                if sameMean:\n",
    "                    normalizedTimeSeries[column] = normalizedTimeSeries[\n",
    "                                                       column] / normalizedTimeSeries[column].mean()\n",
    "            else:\n",
    "                normalizedTimeSeries[column] = self.timeSeries[column]\n",
    "        return normalizedTimeSeries\n",
    "\n",
    "    def _unnormalizeTimeSeries(self, normalizedTimeSeries, sameMean=False):\n",
    "        '''\n",
    "        Equivalent to '_normalizeTimeSeries'. Just does the back\n",
    "        transformation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        normalizedTimeSeries: pandas.DataFrame(), required\n",
    "            Time series which should get back transformated.\n",
    "        sameMean: boolean, optional (default: False)\n",
    "            Has to have the same value as in _normalizeTimeSeries.\n",
    "        '''\n",
    "        unnormalizedTimeSeries = pd.DataFrame()\n",
    "        for column in self.timeSeries:\n",
    "            if not self.timeSeries[column].max() == self.timeSeries[column].min():  # ignore constant timeseries\n",
    "                if sameMean:\n",
    "                    unnormalizedTimeSeries[column] = \\\n",
    "                        normalizedTimeSeries[column] * \\\n",
    "                        (self.timeSeries[column].mean() -\n",
    "                         self.timeSeries[column].min()) / \\\n",
    "                        (self.timeSeries[column].max() -\n",
    "                         self.timeSeries[column].min())\n",
    "                else:\n",
    "                    unnormalizedTimeSeries[\n",
    "                        column] = normalizedTimeSeries[column]\n",
    "                unnormalizedTimeSeries[column] = \\\n",
    "                    unnormalizedTimeSeries[column] * \\\n",
    "                    (self.timeSeries[column].max() -\n",
    "                     self.timeSeries[column].min()) + \\\n",
    "                    self.timeSeries[column].min()\n",
    "            else:\n",
    "                unnormalizedTimeSeries[column] = normalizedTimeSeries[column]\n",
    "        return unnormalizedTimeSeries\n",
    "\n",
    "    def _preProcessTimeSeries(self):\n",
    "        '''\n",
    "        Normalize the time series, weight them based on the weight dict and\n",
    "        puts them into the correct matrix format.\n",
    "        '''\n",
    "        # normalize the time series and group them to periodly profiles\n",
    "        self.normalizedTimeSeries = self._normalizeTimeSeries(\n",
    "            sameMean=self.sameMean)\n",
    "\n",
    "        for column in self.weightDict:\n",
    "            if self.weightDict[column] == 0:\n",
    "                self.normalizedTimeSeries[\n",
    "                    column] = self.normalizedTimeSeries[column] * 0.0001\n",
    "            else:\n",
    "                self.normalizedTimeSeries[column] = self.normalizedTimeSeries[\n",
    "                                                        column] * self.weightDict[column]\n",
    "\n",
    "        self.normalizedPeriodlyProfiles, self.timeIndex = unstackToPeriods(\n",
    "            self.normalizedTimeSeries, self.timeStepsPerPeriod)\n",
    "\n",
    "        # check if no NaN is in the resulting profiles\n",
    "        if self.normalizedPeriodlyProfiles.isnull().values.any():\n",
    "            raise ValueError(\n",
    "                'Pre processed data includes NaN. Please check the timeSeries input data.')\n",
    "\n",
    "    def _postProcessTimeSeries(self, normalizedTimeSeries):\n",
    "        '''\n",
    "        Neutralizes the weighting the time series back and unnormalizes them.\n",
    "        '''\n",
    "        for column in self.weightDict:\n",
    "            if self.weightDict[column] == 0:\n",
    "                normalizedTimeSeries[\n",
    "                    column] = normalizedTimeSeries[column] / 0.0001\n",
    "            else:\n",
    "                normalizedTimeSeries[column] = normalizedTimeSeries[\n",
    "                                                   column] / self.weightDict[column]\n",
    "\n",
    "        unnormalizedTimeSeries = self._unnormalizeTimeSeries(\n",
    "            normalizedTimeSeries, sameMean=self.sameMean)\n",
    "\n",
    "        return unnormalizedTimeSeries\n",
    "\n",
    "    def _addExtremePeriods(self, groupedSeries, clusterCenters,\n",
    "                           clusterOrder,\n",
    "                           extremePeriodMethod='new_cluster_center',\n",
    "                           addPeakMin=None,\n",
    "                           addPeakMax=None,\n",
    "                           addMeanMin=None,\n",
    "                           addMeanMax=None):\n",
    "        '''\n",
    "        Adds different extreme periods based on the to the clustered data,\n",
    "        decribed by the clusterCenters and clusterOrder.\n",
    "        Parameters\n",
    "        ----------\n",
    "        groupedSeries: pandas.DataFrame(), required\n",
    "            periodly grouped groupedSeries on which basis it should be decided,\n",
    "            which period is an extreme period.\n",
    "        clusterCenters: dict, required\n",
    "            Output from clustering with sklearn.\n",
    "        clusterOrder: dict, required\n",
    "            Output from clsutering with sklearn.\n",
    "        extremePeriodMethod: str, optional(default: 'new_cluster_center' )\n",
    "            Chosen extremePeriodMethod. The method\n",
    "        Returns\n",
    "        -------\n",
    "        newClusterCenters\n",
    "            The new cluster centers extended with the extreme periods.\n",
    "        newClusterOrder\n",
    "            The new cluster order including the extreme periods.\n",
    "        extremeClusterIdx\n",
    "            A list of indices where in the newClusterCenters are the extreme\n",
    "            periods located.\n",
    "        '''\n",
    "\n",
    "        # init required dicts and lists\n",
    "        if addPeakMin is None:\n",
    "            addPeakMin = []\n",
    "        if addPeakMax is None:\n",
    "            addPeakMax = []\n",
    "        if addMeanMin is None:\n",
    "            addMeanMin = []\n",
    "        if addMeanMax is None:\n",
    "            addMeanMax = []\n",
    "        self.extremePeriods = {}\n",
    "        extremePeriodNo = []\n",
    "\n",
    "        ccList = [center.tolist() for center in clusterCenters]\n",
    "\n",
    "        # check which extreme periods exist in the profile and add them to\n",
    "        # self.extremePeriods dict\n",
    "        for column in self.timeSeries.columns:\n",
    "\n",
    "            if column in addPeakMax:\n",
    "                stepNo = groupedSeries[column].max(axis=1).idxmax()\n",
    "                # add only if stepNo is not already in extremePeriods\n",
    "                # if it is not already a cluster center\n",
    "                if stepNo not in extremePeriodNo and groupedSeries.ix[stepNo].values.tolist() not in ccList:\n",
    "                    self.extremePeriods[column + ' max.'] = \\\n",
    "                        {'stepNo': stepNo,\n",
    "                         'profile': groupedSeries.ix[stepNo].values,\n",
    "                         'column': column}\n",
    "                    extremePeriodNo.append(stepNo)\n",
    "\n",
    "            if column in addPeakMin:\n",
    "                stepNo = groupedSeries[column].min(axis=1).idxmin()\n",
    "                # add only if stepNo is not already in extremePeriods\n",
    "                # if it is not already a cluster center\n",
    "                if stepNo not in extremePeriodNo and groupedSeries.ix[stepNo].values.tolist() not in ccList:\n",
    "                    self.extremePeriods[column + ' min.'] = \\\n",
    "                        {'stepNo': stepNo,\n",
    "                         'profile': groupedSeries.ix[stepNo].values,\n",
    "                         'column': column}\n",
    "                    extremePeriodNo.append(stepNo)\n",
    "\n",
    "            if column in addMeanMax:\n",
    "                stepNo = groupedSeries[column].mean(axis=1).idxmax()\n",
    "                # add only if stepNo is not already in extremePeriods\n",
    "                # if it is not already a cluster center\n",
    "                if stepNo not in extremePeriodNo and groupedSeries.ix[stepNo].values.tolist() not in ccList:\n",
    "                    self.extremePeriods[column + ' daily min.'] = \\\n",
    "                        {'stepNo': stepNo,\n",
    "                         'profile': groupedSeries.ix[stepNo].values,\n",
    "                         'column': column}\n",
    "                    extremePeriodNo.append(stepNo)\n",
    "\n",
    "            if column in addMeanMin:\n",
    "                stepNo = groupedSeries[column].mean(axis=1).idxmin()\n",
    "                # add only if stepNo is not already in extremePeriods and\n",
    "                # if it is not already a cluster center\n",
    "                if stepNo not in extremePeriodNo and groupedSeries.ix[stepNo].values.tolist() not in ccList:\n",
    "                    self.extremePeriods[column + ' daily min.'] = \\\n",
    "                        {'stepNo': stepNo,\n",
    "                         'profile': groupedSeries.ix[stepNo].values,\n",
    "                         'column': column}\n",
    "                    extremePeriodNo.append(stepNo)\n",
    "\n",
    "        for periodType in self.extremePeriods:\n",
    "            # get current related clusters of extreme periods\n",
    "            self.extremePeriods[periodType]['clusterNo'] = clusterOrder[\n",
    "                self.extremePeriods[periodType]['stepNo']]\n",
    "\n",
    "            # init new cluster structure\n",
    "        newClusterCenters = []\n",
    "        newClusterOrder = clusterOrder\n",
    "        extremeClusterIdx = []\n",
    "\n",
    "        # integrate extreme periods to clusters\n",
    "        if extremePeriodMethod == 'append':\n",
    "            # attach extreme periods to cluster centers\n",
    "            for i, cluster_center in enumerate(clusterCenters):\n",
    "                newClusterCenters.append(cluster_center)\n",
    "            for i, periodType in enumerate(self.extremePeriods):\n",
    "                extremeClusterIdx.append(len(newClusterCenters))\n",
    "                newClusterCenters.append(\n",
    "                    self.extremePeriods[periodType]['profile'])\n",
    "                newClusterOrder[self.extremePeriods[periodType]['stepNo']] = i + len(clusterCenters)\n",
    "\n",
    "        elif extremePeriodMethod == 'new_cluster_center':\n",
    "            for i, cluster_center in enumerate(clusterCenters):\n",
    "                newClusterCenters.append(cluster_center)\n",
    "            # attach extrem periods to cluster centers and consider for all periods\n",
    "            # if the fit better to the cluster or the extrem period\n",
    "            for i, periodType in enumerate(self.extremePeriods):\n",
    "                extremeClusterIdx.append(len(newClusterCenters))\n",
    "                newClusterCenters.append(\n",
    "                    self.extremePeriods[periodType]['profile'])\n",
    "                self.extremePeriods[periodType][\n",
    "                    'newClusterNo'] = i + len(clusterCenters)\n",
    "\n",
    "            for i, cPeriod in enumerate(newClusterOrder):\n",
    "                # caclulate euclidean distance to cluster center\n",
    "                cluster_dist = sum(\n",
    "                    (groupedSeries.ix[i].values - clusterCenters[cPeriod]) ** 2)\n",
    "                for ii, extremPeriodType in enumerate(self.extremePeriods):\n",
    "                    # exclude other extreme periods from adding to the new\n",
    "                    # cluster center\n",
    "                    isOtherExtreme = False\n",
    "                    for otherExPeriod in self.extremePeriods:\n",
    "                        if (i == self.extremePeriods[otherExPeriod]['stepNo']\n",
    "                                and otherExPeriod != extremPeriodType):\n",
    "                            isOtherExtreme = True\n",
    "                    # calculate distance to extreme periods\n",
    "                    extperiod_dist = sum(\n",
    "                        (groupedSeries.ix[i].values -\n",
    "                         self.extremePeriods[extremPeriodType]['profile']) ** 2)\n",
    "                    # choose new cluster relation\n",
    "                    if extperiod_dist < cluster_dist and not isOtherExtreme:\n",
    "                        newClusterOrder[i] = self.extremePeriods[\n",
    "                            extremPeriodType]['newClusterNo']\n",
    "\n",
    "        elif extremePeriodMethod == 'replace_cluster_center':\n",
    "            # Worst Case Clusterperiods\n",
    "            newClusterCenters = clusterCenters\n",
    "            for periodType in self.extremePeriods:\n",
    "                index = groupedSeries.columns.get_loc(\n",
    "                    self.extremePeriods[periodType]['column'])\n",
    "                newClusterCenters[self.extremePeriods[periodType]['clusterNo']][index] = \\\n",
    "                    self.extremePeriods[periodType]['profile'][index]\n",
    "                if not self.extremePeriods[periodType]['clusterNo'] in extremeClusterIdx:\n",
    "                    extremeClusterIdx.append(\n",
    "                        self.extremePeriods[periodType]['clusterNo'])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Chosen \"extremePeriodMethod\": ' +\n",
    "                                      str(extremePeriodMethod) + ' is ' +\n",
    "                                      'not implemented.')\n",
    "\n",
    "        return newClusterCenters, newClusterOrder, extremeClusterIdx\n",
    "\n",
    "    def _rescaleClusterPeriods(\n",
    "            self, clusterOrder, clusterPeriods, extremeClusterIdx):\n",
    "        '''\n",
    "        Rescale the values of the clustered Periods such that mean of each time\n",
    "        series in the typical Periods fits the mean value of the original time\n",
    "        series, without changing the values of the extremePeriods.\n",
    "        '''\n",
    "        weightingVec = pd.Series(self._clusterPeriodNoOccur).values\n",
    "        typicalPeriods = pd.DataFrame(\n",
    "            clusterPeriods, columns=self.normalizedPeriodlyProfiles.columns)\n",
    "        idx_wo_peak = np.delete(typicalPeriods.index, extremeClusterIdx)\n",
    "        for column in self.timeSeries.columns:\n",
    "            diff = 1\n",
    "            sum_raw = self.normalizedPeriodlyProfiles[column].sum().sum()\n",
    "            sum_peak = sum(\n",
    "                weightingVec[extremeClusterIdx] *\n",
    "                typicalPeriods[column].loc[extremeClusterIdx,:].sum(\n",
    "                    axis=1))\n",
    "            sum_clu_wo_peak = sum(\n",
    "                weightingVec[idx_wo_peak] *\n",
    "                typicalPeriods[column].loc[idx_wo_peak,:].sum(\n",
    "                    axis=1))\n",
    "\n",
    "            # define the upper scale dependent on the weighting of the series\n",
    "            scale_ub = 1.0\n",
    "            if self.sameMean:\n",
    "                scale_ub = scale_ub * self.timeSeries[column].max(\n",
    "                ) / self.timeSeries[column].mean()\n",
    "            if column in self.weightDict:\n",
    "                scale_ub = scale_ub * self.weightDict[column]\n",
    "\n",
    "            # difference between predicted and original sum\n",
    "            diff = abs(sum_raw - (sum_clu_wo_peak + sum_peak))\n",
    "\n",
    "            # use while loop to rescale cluster periods\n",
    "            a = 0\n",
    "            while diff > sum_raw * TOLERANCE and a < MAX_ITERATOR:\n",
    "                # rescale values\n",
    "                typicalPeriods.loc[idx_wo_peak, column] = \\\n",
    "                    (typicalPeriods[column].loc[idx_wo_peak,:].values *\n",
    "                     (sum_raw - sum_peak) / sum_clu_wo_peak)\n",
    "\n",
    "                # reset values higher than the upper sacle or less than zero\n",
    "                typicalPeriods[column][\n",
    "                    typicalPeriods[column] > scale_ub] = scale_ub\n",
    "                typicalPeriods[column][\n",
    "                    typicalPeriods[column] < 0.0] = 0.0\n",
    "\n",
    "                typicalPeriods[column] = typicalPeriods[\n",
    "                    column].fillna(0.0)\n",
    "\n",
    "                # calc new sum and new diff to orig data\n",
    "                sum_clu_wo_peak = sum(\n",
    "                    weightingVec[idx_wo_peak] *\n",
    "                    typicalPeriods[column].loc[idx_wo_peak,:].sum(\n",
    "                        axis=1))\n",
    "                diff = abs(sum_raw - (sum_clu_wo_peak + sum_peak))\n",
    "                a += 1\n",
    "            if a == MAX_ITERATOR:\n",
    "                deviation = str(round((diff/sum_raw)*100,2))\n",
    "                warnings.warn(\n",
    "                    'Max iteration number reached for \"'+ str(column)\n",
    "                    +'\" while rescaling the cluster periods.' + \n",
    "                    ' The integral of the aggregated time series deviates by: ' \n",
    "                    + deviation + '%')\n",
    "        return typicalPeriods.values\n",
    "\n",
    "    def _clusterSortedPeriods(self, candidates, n_init=20):\n",
    "        '''\n",
    "        Runs the clustering algorithms for the sorted profiles within the period\n",
    "        instead of the original profiles. (Duration curve clustering)\n",
    "        '''\n",
    "        # initialize\n",
    "        normalizedSortedPeriodlyProfiles = copy.deepcopy(\n",
    "            self.normalizedPeriodlyProfiles)\n",
    "        for column in self.timeSeries.columns:\n",
    "            # sort each period individually\n",
    "            df = normalizedSortedPeriodlyProfiles[column]\n",
    "            values = df.values\n",
    "            values.sort(axis=1)\n",
    "            values = values[:, ::-1]\n",
    "            normalizedSortedPeriodlyProfiles[column] = pd.DataFrame(\n",
    "                values, df.index, df.columns)\n",
    "        sortedClusterValues = normalizedSortedPeriodlyProfiles.values\n",
    "\n",
    "        clusterOrders_iter = []\n",
    "        clusterCenters_iter = []\n",
    "        distanceMedoid_iter = []\n",
    "\n",
    "        for i in range(n_init):\n",
    "            altClusterCenters, clusterCenterIndices, clusterOrders_C = (\n",
    "                aggregatePeriods(\n",
    "                    sortedClusterValues, n_clusters=self.noTypicalPeriods,\n",
    "                    n_iter=30, solver=self.solver,\n",
    "                    clusterMethod=self.clusterMethod))\n",
    "\n",
    "            clusterCenters_C = []\n",
    "            distanceMedoid_C = []\n",
    "\n",
    "            # take the clusters and determine the most representative sorted\n",
    "            # period as cluster center\n",
    "            for clusterNum in np.unique(clusterOrders_C):\n",
    "                indice = np.where(clusterOrders_C == clusterNum)[0]\n",
    "                if len(indice) > 1:\n",
    "                    # mean value for each time step for each time series over\n",
    "                    # all Periods in the cluster\n",
    "                    currentMean_C = sortedClusterValues[indice].mean(axis=0)\n",
    "                    # index of the period with the lowest distance to the cluster\n",
    "                    # center\n",
    "                    mindistIdx_C = np.argmin(\n",
    "                        np.square(\n",
    "                            sortedClusterValues[indice] -\n",
    "                            currentMean_C).sum(\n",
    "                            axis=1))\n",
    "                    # append original time series of this period\n",
    "                    medoid_C = candidates[indice][mindistIdx_C]\n",
    "\n",
    "                    # append to cluster center\n",
    "                    clusterCenters_C.append(medoid_C)\n",
    "\n",
    "                    # calculate metrix for evaluation\n",
    "                    distanceMedoid_C.append(\n",
    "                        abs(candidates[indice] - medoid_C).sum())\n",
    "\n",
    "                else:\n",
    "                    # if only on period is part of the cluster, add this index\n",
    "                    clusterCenters_C.append(candidates[indice][0])\n",
    "                    distanceMedoid_C.append(0)\n",
    "\n",
    "            # collect matrix\n",
    "            distanceMedoid_iter.append(abs(sum(distanceMedoid_C)))\n",
    "            clusterCenters_iter.append(clusterCenters_C)\n",
    "            clusterOrders_iter.append(clusterOrders_C)\n",
    "\n",
    "        bestFit = np.argmin(distanceMedoid_iter)\n",
    "\n",
    "        return clusterCenters_iter[bestFit], clusterOrders_iter[bestFit]\n",
    "\n",
    "    def createTypicalPeriods(self):\n",
    "        '''\n",
    "        Clusters the Periods.\n",
    "        Returns\n",
    "        -------\n",
    "        self.clusterPeriods\n",
    "            All typical Periods in scaled form.\n",
    "        '''\n",
    "        self._preProcessTimeSeries()\n",
    "\n",
    "        # check for additional cluster parameters\n",
    "        if self.evalSumPeriods:\n",
    "            evaluationValues = self.normalizedPeriodlyProfiles.stack(\n",
    "                level=0).sum(\n",
    "                axis=1).unstack(\n",
    "                level=1)\n",
    "            # how many values have to get deleted later\n",
    "            delClusterParams = -len(evaluationValues.columns)\n",
    "            candidates = np.concatenate(\n",
    "                (self.normalizedPeriodlyProfiles.values, evaluationValues.values),\n",
    "                axis=1)\n",
    "        else:\n",
    "            delClusterParams = None\n",
    "            candidates = self.normalizedPeriodlyProfiles.values\n",
    "\n",
    "        cluster_duration = time.time()\n",
    "        if not self.sortValues:\n",
    "            # cluster the data\n",
    "            self.clusterCenters, self.clusterCenterIndices, \\\n",
    "                self._clusterOrder = aggregatePeriods(\n",
    "                candidates, n_clusters=self.noTypicalPeriods, n_iter=100,\n",
    "                solver=self.solver, clusterMethod=self.clusterMethod)\n",
    "        else:\n",
    "            self.clusterCenters, self._clusterOrder = self._clusterSortedPeriods(\n",
    "                candidates)\n",
    "        self.clusteringDuration = time.time() - cluster_duration\n",
    "\n",
    "        # get cluster centers without additional evaluation values\n",
    "        self.clusterPeriods = []\n",
    "        for i, cluster_center in enumerate(self.clusterCenters):\n",
    "            self.clusterPeriods.append(cluster_center[:delClusterParams])\n",
    "\n",
    "        if not self.extremePeriodMethod == 'None':\n",
    "            # overwrite clusterPeriods and clusterOrder\n",
    "            self.clusterPeriods, self._clusterOrder, self.extremeClusterIdx = \\\n",
    "                self._addExtremePeriods(self.normalizedPeriodlyProfiles,\n",
    "                                        self.clusterPeriods,\n",
    "                                        self._clusterOrder,\n",
    "                                        extremePeriodMethod=self.extremePeriodMethod,\n",
    "                                        addPeakMin=self.addPeakMin,\n",
    "                                        addPeakMax=self.addPeakMax,\n",
    "                                        addMeanMin=self.addMeanMin,\n",
    "                                        addMeanMax=self.addMeanMax)\n",
    "        else:\n",
    "            self.extremeClusterIdx = []\n",
    "\n",
    "        # get number of appearance of the the typical periods\n",
    "        nums, counts = np.unique(self._clusterOrder, return_counts=True)\n",
    "        self._clusterPeriodNoOccur = {num: counts[ii] for ii, num in enumerate(nums)}\n",
    "\n",
    "        if self.rescaleClusterPeriods:\n",
    "            self.clusterPeriods = self._rescaleClusterPeriods(\n",
    "                self._clusterOrder, self.clusterPeriods, self.extremeClusterIdx)\n",
    "\n",
    "        # if additional time steps have been added, reduce the number of occurance of the typical period\n",
    "        # which is related to these time steps\n",
    "        if not len(self.timeSeries) % self.timeStepsPerPeriod == 0:\n",
    "            self._clusterPeriodNoOccur[self._clusterOrder[-1]] -= (\n",
    "                1 - float(len(self.timeSeries) % self.timeStepsPerPeriod) / self.timeStepsPerPeriod)\n",
    "\n",
    "        # put the clustered data in pandas format and scale back\n",
    "        clustered_data_raw = pd.DataFrame(\n",
    "            self.clusterPeriods,\n",
    "            columns=self.normalizedPeriodlyProfiles.columns).stack(\n",
    "            level='TimeStep')\n",
    "\n",
    "        self.typicalPeriods = self._postProcessTimeSeries(clustered_data_raw)\n",
    "\n",
    "        return self.typicalPeriods\n",
    "\n",
    "    def prepareEnersysInput(self):\n",
    "        '''\n",
    "        Creates all dictionaries and lists which are required for the energysystem\n",
    "        optimization input.\n",
    "        '''\n",
    "        warnings.warn(\n",
    "            '\"prepareEnersysInput\" is deprecated, since the created attributes can be directly accessed as properties',\n",
    "            DeprecationWarning)\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def stepIdx(self):\n",
    "        '''\n",
    "        Index inside a single cluster\n",
    "        '''\n",
    "        return [ix for ix in range(0, self.timeStepsPerPeriod)]\n",
    "\n",
    "    @property\n",
    "    def clusterPeriodIdx(self):\n",
    "        '''\n",
    "        Index of the clustered periods\n",
    "        '''\n",
    "        if not hasattr(self, 'clusterOrder'):\n",
    "            self.createTypicalPeriods()\n",
    "        return np.sort(np.unique(self._clusterOrder))\n",
    "\n",
    "    @property\n",
    "    def clusterOrder(self):\n",
    "        '''\n",
    "        How often does an typical period occure in the original time series\n",
    "        '''\n",
    "        if not hasattr(self, '_clusterOrder'):\n",
    "            self.createTypicalPeriods()\n",
    "        return self._clusterOrder\n",
    "\n",
    "    @property\n",
    "    def clusterPeriodNoOccur(self):\n",
    "        '''\n",
    "        How often does an typical period occure in the original time series\n",
    "        '''\n",
    "        if not hasattr(self, 'clusterOrder'):\n",
    "            self.createTypicalPeriods()\n",
    "        return self._clusterPeriodNoOccur\n",
    "\n",
    "    @property\n",
    "    def clusterPeriodDict(self):\n",
    "        '''\n",
    "        Time series data for each period index as dictionary\n",
    "        '''\n",
    "        if not hasattr(self, '_clusterOrder'):\n",
    "            self.createTypicalPeriods()\n",
    "        if not hasattr(self, '_clusterPeriodDict'):\n",
    "            self._clusterPeriodDict = {}\n",
    "            for column in self.typicalPeriods:\n",
    "                self._clusterPeriodDict[column] = self.typicalPeriods[column].to_dict()\n",
    "        return self._clusterPeriodDict\n",
    "\n",
    "    def predictOriginalData(self):\n",
    "        '''\n",
    "        Predicts the overall time series if every period would be placed in the\n",
    "        related cluster center\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            DataFrame which has the same shape as the original one.\n",
    "        '''\n",
    "        if not hasattr(self, '_clusterOrder'):\n",
    "            self.createTypicalPeriods()\n",
    "\n",
    "        new_data = []\n",
    "        for label in self._clusterOrder:\n",
    "            new_data.append(self.clusterPeriods[label])\n",
    "\n",
    "        # back in matrix\n",
    "        clustered_data_df = \\\n",
    "            pd.DataFrame(new_data,\n",
    "                         columns=self.normalizedPeriodlyProfiles.columns,\n",
    "                         index=self.normalizedPeriodlyProfiles.index)\n",
    "        clustered_data_df = clustered_data_df.stack(level='TimeStep')\n",
    "\n",
    "        # back in form\n",
    "        self.normalizedPredictedData = \\\n",
    "            pd.DataFrame(clustered_data_df.values[:len(self.timeSeries)],\n",
    "                         index=self.timeSeries.index,\n",
    "                         columns=self.timeSeries.columns)\n",
    "        self.predictedData = self._postProcessTimeSeries(\n",
    "            self.normalizedPredictedData)\n",
    "\n",
    "        return self.predictedData\n",
    "\n",
    "    def indexMatching(self):\n",
    "        '''\n",
    "        Relates the index of the original time series with the indices\n",
    "        represented by the clusters\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            DataFrame which has the same shape as the original one.\n",
    "        '''\n",
    "        if not hasattr(self, '_clusterOrder'):\n",
    "            self.createTypicalPeriods()\n",
    "\n",
    "        # create aggregated period and timestep index lists\n",
    "        periodIndex = []\n",
    "        stepIndex = []\n",
    "        for label in self._clusterOrder:\n",
    "            for step in range(self.timeStepsPerPeriod):\n",
    "                periodIndex.append(label)\n",
    "                stepIndex.append(step)\n",
    "\n",
    "        # create a dataframe\n",
    "        timeStepMatching = \\\n",
    "            pd.DataFrame([periodIndex, stepIndex],\n",
    "                         index=['PeriodNum', 'TimeStep'],\n",
    "                         columns=self.timeIndex).T\n",
    "\n",
    "        return timeStepMatching\n",
    "\n",
    "    def accuracyIndicators(self):\n",
    "        '''\n",
    "        Compares the predicted data with the orginal time series.\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Dataframe containing indicators evaluating the accuracy of the\n",
    "            aggregation\n",
    "        '''\n",
    "        if not hasattr(self, 'predictedData'):\n",
    "            self.predictOriginalData()\n",
    "\n",
    "        indicatorRaw = {\n",
    "            'RMSE': {},\n",
    "            'RMSE_duration': {},\n",
    "            'MAE': {}}  # 'Silhouette score':{},\n",
    "\n",
    "        for column in self.normalizedTimeSeries.columns:\n",
    "            origTS = self.normalizedTimeSeries[column]\n",
    "            predTS = self.normalizedPredictedData[column]\n",
    "            indicatorRaw['RMSE'][column] = np.sqrt(\n",
    "                mean_squared_error(origTS, predTS))\n",
    "            indicatorRaw['RMSE_duration'][column] = np.sqrt(mean_squared_error(\n",
    "                origTS.sort_values(ascending=False).reset_index(drop=True),\n",
    "                predTS.sort_values(ascending=False).reset_index(drop=True)))\n",
    "            indicatorRaw['MAE'][column] = mean_absolute_error(origTS, predTS)\n",
    "\n",
    "        return pd.DataFrame(indicatorRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df = pd.read_csv('loads-p_set.csv', index_col = 0)\n",
    "typical_days = 5\n",
    "segments_per_day = 24\n",
    "hours = 24\n",
    "clusterMethod = 'hierarchical'\n",
    "segmentation = True\n",
    "aggr=[]\n",
    "chron=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other adjacent clustering\n",
    "aggregation = TimeSeriesAggregation(\n",
    "        timeseries_df,\n",
    "        noTypicalPeriods=typical_days,\n",
    "        hoursPerPeriod=hours,\n",
    "        clusterMethod= clusterMethod)\n",
    "timeseries = aggregation.createTypicalPeriods()\n",
    "timeseries_chron2 = aggregation.predictOriginalData()\n",
    "chron.append(timeseries_chron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x200 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x200 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "param = '12'\n",
    "def plotTS(data, periodlength, vmin, vmax, label = 'T [°C]'):\n",
    "    fig, axes = plt.subplots(figsize = [6, 2], dpi = 100, nrows = 1, ncols = 1)\n",
    "    stacked, timeindex = unstackToPeriods(copy.deepcopy(data), periodlength)\n",
    "    cax = axes.imshow(stacked.values.T, interpolation = 'nearest', vmin = vmin, vmax = vmax)\n",
    "    axes.set_aspect('auto')  \n",
    "    axes.set_ylabel('Hour')\n",
    "    plt.xlabel('Day')\n",
    "    fig.subplots_adjust(right = 1.2)\n",
    "    cbar=plt.colorbar(cax)    \n",
    "    cbar.set_label(label)\n",
    "\n",
    "plotTS(timeseries_df[param], 24, vmin = timeseries_df[param].min(), vmax = timeseries_df[param].max(), label = param + \", raw\"\n",
    ")\n",
    "plotTS(timeseries_chron2[param], 24, vmin = timeseries_df[param].min(), vmax = timeseries_df[param].max(), label = param + \", chron2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
